{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Assessment 3\n",
    "# Task 2 Generate Sparse Representations \n",
    "#### Student Name: Syed Ali Alim Rizvi\n",
    "#### Student ID: 28984773\n",
    "\n",
    "Date: 3/06/2018\n",
    "\n",
    "Version: 3.0\n",
    "\n",
    "Environment: Python 3.6.0 and Anaconda 4.3.0 (64-bit)\n",
    "\n",
    "Libraries used:\n",
    "* from bs4 BeautifulSoup\n",
    "* re \n",
    "* os\n",
    "* pandas\n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "This assignment comprises the creation of vocabulary, and sparse represention of the files in task 1 as well as it segment and topic boundary boolean representation. \n",
    "\n",
    "Tasks:\n",
    "1. Importing libraries\n",
    "2. Reading data in\n",
    "3. Creating the files\n",
    "\n",
    "More details for each task will be given in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "from collections import Counter #for counting elements in list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenizing files and creating vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usint a regex tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer regex\n",
    "tokenizer = RegexpTokenizer(r\"\\w+(?:[-']\\w+)?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### importing the stopwords file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords file\n",
    "stop_words_file = open('stopwords_en.txt', 'r')\n",
    "stop_words = stop_words_file.read()\n",
    "stop_words = stop_words.split('\\n')\n",
    "stop_words_file.close()\n",
    "\n",
    "#stop words set\n",
    "stop_words_set = set(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokens\n",
    "creating unique tokens for words with lengths >=3 for each file and then removing the stop words and the words which occur in more than 132 documents to create vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dict = {}\n",
    "\n",
    "#folder to check\n",
    "xml_file_path = './txt_files'\n",
    "\n",
    "#all topic files\n",
    "files = os.listdir(xml_file_path)\n",
    "\n",
    "#for each topic file \n",
    "for xfile in files:\n",
    "    \n",
    "    #skip unrequired files\n",
    "    if xfile == 'example_output.txt':\n",
    "        continue\n",
    "        \n",
    "    topic_id = xfile[:-4]\n",
    "    \n",
    "    #create the path to file\n",
    "    xfile = os.path.join(xml_file_path, xfile)\n",
    "    \n",
    "    #if the file is a correct file and is .xml\n",
    "    if os.path.isfile(xfile) and xfile.endswith('.txt'): \n",
    "        \n",
    "        #read the raw text in\n",
    "        my_file = open(xfile, 'r')\n",
    "        raw_text = my_file.read().lower().strip()\n",
    "        \n",
    "        #create unigram tokens\n",
    "        unigram_tokens = tokenizer.tokenize(raw_text)\n",
    "        \n",
    "        #create list counts \n",
    "        unigram_tokens_count = Counter(unigram_tokens)\n",
    "        \n",
    "        #unique tokens set\n",
    "        unigram_tokens_set = set(unigram_tokens)\n",
    "        unigram_tokens_set = set([x for x in unigram_tokens_set if len(x)>2])\n",
    "        \n",
    "        #unigram without stopwords\n",
    "        unigram_tokens_wo_sw = unigram_tokens_set - stop_words_set\n",
    "        unigram_tokens_wo_sw = list(unigram_tokens_wo_sw)\n",
    "        \n",
    "        #store in token dictionary\n",
    "        token_dict[topic_id] = unigram_tokens_wo_sw  \n",
    "        \n",
    "        #close file\n",
    "        my_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create on token list from all unique tokens collected and creating a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all tokens\n",
    "all_tokens_list = []\n",
    "\n",
    "# store all tokens from all files in one list\n",
    "for tokens in token_dict.values():\n",
    "    all_tokens_list.extend(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting counts of tokens\n",
    "all_tokens_counts = Counter(all_tokens_list)\n",
    "all_tokens_counts = dict(all_tokens_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting only clean tokens\n",
    "all_tokens_clean = {}\n",
    "all_tokens_dirty = {}\n",
    "\n",
    "# for each word and its freq accrose files\n",
    "for key, value in all_tokens_counts.items():\n",
    "    # if freq is > 132 add it to the dirty dict\n",
    "    if value>132:\n",
    "        all_tokens_dirty[key] = value\n",
    "    # otherwise add it to the clean dict\n",
    "    else:\n",
    "        all_tokens_clean[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting clean token list\n",
    "clean_tokens = [x for x in all_tokens_clean.keys()]\n",
    "clean_tokens.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the vocab\n",
    "vocab_dict = {}\n",
    "vocab = ''\n",
    "\n",
    "#ofr each word in the cleaned token list create a dictionary and a string \n",
    "for ind, word in enumerate(clean_tokens):\n",
    "    vocab_dict[word] = ind\n",
    "    vocab = vocab + '{}:{}\\n'.format(word, ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write vocab file using the vocab string\n",
    "vocab_text_file = open('vocab.txt', 'w')\n",
    "vocab_text_file.write(vocab)\n",
    "vocab_text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10318 --Total Tokens without Stop words\n",
      "13 --Dirty Tokens >132\n",
      "10305 --Clean Tokens <132\n"
     ]
    }
   ],
   "source": [
    "#print for checking\n",
    "print(len(all_tokens_counts), '--Total Tokens without Stop words')\n",
    "print(len(all_tokens_dirty), '--Dirty Tokens >132')\n",
    "print(len(all_tokens_clean), '--Clean Tokens <132')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. creating boolean representaton and the sparse files for each topic. \n",
    "\n",
    "The output of task1 is used to create this. \n",
    "\n",
    "this is because if a root topic ends in the middle of a segment, the first half of the segment would be a one in the boolean and the next would be zero. However if the XML version is used to create the sparse and boolean representation the end of root topic would be missed and hence a 1 will not be shown and then it will not remain consistent with the sparse file. \n",
    "\n",
    "Hence for creating the sparse file the output of task 1 is used, i.e. if a segment is broken due to an ending of topic in the middle of it, the broken segments are used to sparse the file and create a boolean representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#store the topic segments boolean representation after preprosessing\n",
    "topic_seg=''\n",
    "\n",
    "#folder to check\n",
    "xml_file_path = './txt_files'\n",
    "\n",
    "#all topic files\n",
    "files = os.listdir(xml_file_path)\n",
    "\n",
    "#for each topic file \n",
    "for xfile in files:\n",
    "    \n",
    "    #skip unrequired files\n",
    "    if xfile == 'example_output.txt':\n",
    "        continue\n",
    "        \n",
    "    topic_id = xfile[:-4]\n",
    "    \n",
    "    #create the path to file\n",
    "    nxfile = os.path.join(xml_file_path, xfile)\n",
    "    \n",
    "    #if the file is a correct file and is .xml\n",
    "    if os.path.isfile(nxfile) and nxfile.endswith('.txt'): \n",
    "        \n",
    "        #for segment boolean storage\n",
    "        segs=''\n",
    "        \n",
    "        #string to store output in \n",
    "        sparse_text = ''\n",
    "        \n",
    "        #read the raw text in\n",
    "        my_file = open(nxfile, 'r')\n",
    "        raw_text = my_file.read().lower()\n",
    "        \n",
    "        #create unigram tokens\n",
    "        segments = raw_text.split('\\n')\n",
    "        \n",
    "        #for each segment in the file\n",
    "        for segment in segments:\n",
    "            \n",
    "            #if the segment is stars put stars in sparse file (they will be removed before writing to file)\n",
    "            if re.search(r'\\*', segment) != None:\n",
    "                sparse_text = sparse_text + '*'*10 + '\\n'\n",
    "                continue\n",
    "                                \n",
    "            #get a list of words\n",
    "            segment_words = tokenizer.tokenize(segment)\n",
    "            #segment_words = segment.split(' ') if we wnated to create sparse files using raw text \n",
    "            \n",
    "            #check count of words\n",
    "            word_counts = dict(Counter(segment_words))\n",
    "            \n",
    "            #for each word and its count \n",
    "            for word, freq in word_counts.items():\n",
    "                #if the word is in the vocab store the id of the word and its freq\n",
    "                if word in list(vocab_dict.keys()):\n",
    "                    word_ind = vocab_dict[word]\n",
    "                    sparse_text += '{}:{},'.format(word_ind, freq)\n",
    "                #otherwise do nothing\n",
    "                else:\n",
    "                    pass\n",
    "            \n",
    "            #add segment to sparse text (if word was added [:-1] would remove comma else would remove \\n that was added)\n",
    "            sparse_text = sparse_text[:-1] + '\\n'\n",
    "        \n",
    "        #remove any leading or trailing white spaces\n",
    "        sparse_text = sparse_text.strip()\n",
    "        \n",
    "        #create topic seg file\n",
    "        segs = re.subn(r'[^\\n|*]', '', sparse_text)[0]\n",
    "        segs = re.subn(r'\\n\\*{10,10}\\n?', '1,', segs)[0]\n",
    "        segs = re.subn(r'\\*{10,10}\\n?', '', segs)[0]\n",
    "        segs = re.subn(r'\\n', '0,', segs)[0]\n",
    "        \n",
    "        #put in topic_seg\n",
    "        topic_seg = topic_seg + '{}:{}\\n'.format(topic_id, segs[:-1])\n",
    "        \n",
    "        #remove stars from sparse file\n",
    "        sparse_text = re.subn(r'\\n\\*{10,10}', '', sparse_text)[0]\n",
    "        sparse_text = re.subn(r'\\*{10,10}\\n', '', sparse_text)[0]\n",
    "        \n",
    "        #create sparse file for topic\n",
    "        out_file_name = './sparse_files/' + xfile\n",
    "        my_output_file = open(out_file_name, 'w')\n",
    "        my_output_file.write(sparse_text.strip())\n",
    "        my_output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### writing the topic segment file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating topics_seg file\n",
    "topic_seg_file = open('topic_seg.txt', 'w')\n",
    "topic_seg_file.write(topic_seg.strip())\n",
    "topic_seg_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this case if the topic boundary cut the segment boundary, then to reflect it in the boolean and sparse representation, the output of task1 was used. \n",
    "- To create the sparse files, the vocab was applied to the tokenized text and not the raw text.\n",
    "- Along with the stopwords and words occuring in more than 132 documents, words less than length 3 were also removed. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
