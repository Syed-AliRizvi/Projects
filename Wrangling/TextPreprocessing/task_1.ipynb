{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Assessment 3\n",
    "# Task 1 Reconstruct the Original Meeting Transcripts\n",
    "#### Student Name: Syed Ali Alim Rizvi\n",
    "#### Student ID: 28984773\n",
    "\n",
    "Date: 3/06/2018\n",
    "\n",
    "Version: 3.0\n",
    "\n",
    "Environment: Python 3.6.0 and Anaconda 4.3.0 (64-bit)\n",
    "\n",
    "Libraries used:\n",
    "* from bs4 BeautifulSoup\n",
    "* re \n",
    "* os\n",
    "* pandas\n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "This assignment comprises the recreation of meeting transcripts from XML files.\n",
    "\n",
    "Tasks:\n",
    "1. Importing libraries\n",
    "2. Reading data in\n",
    "3. Recreation of files\n",
    "\n",
    "More details for each task will be given in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bsoup\n",
    "import re\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing Topic and creating tuples of Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_topic(file):\n",
    "    \n",
    "    #create soup\n",
    "    Topic = bsoup(file, 'lxml',from_encoding=\"ISO-8859-1\")\n",
    "            \n",
    "    root_topics= [] #list of root toppics\n",
    "    parent_dict = {} #identify parent root topic of child \n",
    "    #find all root topics; recursive is false because we dont want children\n",
    "    for i in Topic.find('nite:root').find_all('topic', recursive=False):\n",
    "        root_topic = re.search(r'(?<=\\.)\\d+', i['nite:id']).group(0)\n",
    "        root_topics.append(root_topic)\n",
    "        #check children of root; recursive is true because we want all children no matter how deep\n",
    "        for j in i.find_all('topic'):\n",
    "            child_topic = re.search(r'(?<=\\.)\\d+', j['nite:id']).group(0)\n",
    "            parent_dict[child_topic] = root_topic\n",
    "        \n",
    "    #convert topics into integers\n",
    "    root_topics = [int(x) for x in root_topics]\n",
    "\n",
    "    #creating tuples for topics\n",
    "    tops = []\n",
    "\n",
    "    #create tuples\n",
    "    for topic in Topic.find_all('topic'):\n",
    "        \n",
    "        for child in topic.find_all(re.compile('child')):\n",
    "\n",
    "            #required fields\n",
    "            topic = re.search(r'(?<=\\.)\\d+', child.parent['nite:id']).group(0)\n",
    "            group = re.search(r'\\.(\\w)\\.',child['href']).group(1)\n",
    "            word_lims = re.findall(r'(?<=words)\\d+',child['href'])\n",
    "\n",
    "            #required tuple\n",
    "            if len(word_lims) == 2:\n",
    "                t = tuple([int(topic), group, int(word_lims[0]), int(word_lims[1])])\n",
    "            elif len(word_lims) == 1:\n",
    "                t = tuple([int(topic), group, int(word_lims[0]), int(word_lims[0])])\n",
    "            else:\n",
    "                print('issue')\n",
    "\n",
    "            tops.append(t)\n",
    "        \n",
    "    #return list of tuples, the list of root topics, dictionary of subtopics and their root parents\n",
    "    return (tops, root_topics, parent_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing Segments and creating tuples of Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fucntion to parse a segment\n",
    "def parse_seg(file):\n",
    "    \n",
    "    #create soup\n",
    "    Segment = bsoup(file, 'lxml')\n",
    "    \n",
    "    #list to store the segments tuples\n",
    "    segs=[]\n",
    "\n",
    "    #create tuples\n",
    "    for seg in Segment.find_all(re.compile(r'child')):\n",
    "\n",
    "        #taking out group and word lims\n",
    "        group = re.search(r'\\.(\\w)\\.',seg['href']).group(1)\n",
    "        word_lims = re.findall(r'(?<=words)\\d+',seg['href'])\n",
    "\n",
    "        #required tuple\n",
    "        if len(word_lims) == 2:\n",
    "            t = tuple([group, int(word_lims[0]), int(word_lims[1])])\n",
    "        elif len(word_lims) == 1:\n",
    "            t = tuple([group, int(word_lims[0]), int(word_lims[0])])\n",
    "        else:\n",
    "            print('issue')\n",
    "        \n",
    "        #append tuple\n",
    "        segs.append(t)\n",
    "    \n",
    "    #reutrn list of tuples\n",
    "    return segs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing Words and creating tuples of Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fucntion to parse a words\n",
    "def parse_words(file):\n",
    "    \n",
    "    #create soup\n",
    "    Word = bsoup(file, 'lxml')\n",
    "    \n",
    "    #list to store the segments tuples\n",
    "    words=[]\n",
    "\n",
    "    #create tuples\n",
    "    for wor in Word.find(r'nite:root').find_all():\n",
    "#     for wor in Word.find_all(re.compile(r'w')):\n",
    "#     for wor in Word.find_all(re.compile(r'w|vocal.*')):\n",
    "        #taking out group and word Ind and word    \n",
    "        group = re.search(r'\\.(\\w)\\.',wor['nite:id']).group(1)\n",
    "        try:\n",
    "            word_ind = re.findall(r'.*[^\\d]+(\\d+)', wor['nite:id'])[0]\n",
    "        except IndexError:\n",
    "            print(wor)\n",
    "            print(re.findall(r'(?<=words)\\d+',wor['nite:id']))\n",
    "            raise\n",
    "        word_val = wor.string if wor.string!=None else ''\n",
    "\n",
    "        #required tuple\n",
    "        t = tuple([group, int(word_ind), word_val])\n",
    "\n",
    "        words.append(t)\n",
    "    \n",
    "    #reutrn list of tuples\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to create text output of the Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to create topic\n",
    "def topic_to_text(root_topics, parent_dict, tdf, sdf, wdf):\n",
    "    \n",
    "    ##################################################################################################################\n",
    "    # for the segments data frame add a new column with the segment text\n",
    "    ##################################################################################################################\n",
    "    \n",
    "    # Loop over the segment data base and create text for each segment and add it to the datafrmae\n",
    "    for i in range(len(sdf)):\n",
    "\n",
    "        #identify variables\n",
    "        seg = sdf.iloc[i,0] #which segment it is\n",
    "        start=sdf.iloc[i,1] #what is the start word of the segment\n",
    "        end = sdf.iloc[i,2] #what is the end word of the segment\n",
    "\n",
    "        #create segment\n",
    "        #create a list of words from the words df that are of the same segment and are between start and end of segment\n",
    "        wl = wdf[(wdf.segment == seg) & (wdf.ind >= start) & (wdf.ind <= end)].iloc[:,2].tolist()\n",
    "        #join the words with spaces to create the text for the segment\n",
    "        wstr = ' '.join(wl)\n",
    "\n",
    "        #add the text of the segment to the dataframe column msg\n",
    "        sdf.loc[i,'msg'] = wstr\n",
    "        \n",
    "    \n",
    "    ##################################################################################################################\n",
    "    # for the Topics data frame add a new column with the text for each subtopic\n",
    "    ##################################################################################################################\n",
    "    \n",
    "    #topics\n",
    "    for i in range(len(tdf)):\n",
    "\n",
    "        #identify variables\n",
    "        seg = tdf.iloc[i,1] # segment of the subtopic selected\n",
    "        start = tdf.iloc[i,2] #start word of the subtopic  selected\n",
    "        end = tdf.iloc[i,3] #end word of the subtop selected\n",
    "\n",
    "        #create list of segment texts\n",
    "        not_needed = (sdf.stop < start) | (sdf.start > end) #identify the segments that would NOT be needed for this subtopic\n",
    "        sl = sdf[(sdf.segment == seg) & ~not_needed].iloc[:,3].tolist() #create a list of segments in the topic\n",
    "        \n",
    "        #varible for segment and topic\n",
    "        seg_start = sdf[(sdf.segment == seg) & ~not_needed].iloc[0,1] # what is the starting word of the group of segments\n",
    "        seg_end = sdf[(sdf.segment == seg) & ~not_needed].iloc[-1,2] #what is the ending word of the group of segments\n",
    "        start_diff = int(start-seg_start) #calculate the difference between the segment and topic start words\n",
    "        end_diff = int(seg_end - end) # calculate the difference between the segment and topic end words\n",
    "        \n",
    "        #join the segment texts accordingly\n",
    "        #if topic starts after a segment and ends before a segment aswell (they could be different or same segments)\n",
    "        if (end_diff>0) & (start_diff>0):\n",
    "#             print(end_diff, 'top index', i, '--both')\n",
    "            sl = ' \\n'.join(sl) #sl is a list of segments so we need to join them before indexing (with \\n and a space)\n",
    "            sl = sl.split(' ') #then split them into words by white space (since \\n was with a space so its okay)\n",
    "            sl = sl[start_diff:-end_diff] #select the required words\n",
    "            sl = ' '.join(sl) #join them with spaces\n",
    "            sstr = sl\n",
    "        #if the topic ends before a segment\n",
    "        elif end_diff>0:\n",
    "            sl = ' \\n'.join(sl) #sl is a list of segments so we need to join them before indexing (with \\n and a space)\n",
    "            sl = sl.split(' ') #then split them into words by white space (since \\n was with a space so its okay)\n",
    "            sl = sl[:-end_diff] #select the required words\n",
    "            sl = ' '.join(sl) #join them with spaces\n",
    "            sstr = sl\n",
    "        #if a topic starts after a segment \n",
    "        elif start_diff>0:\n",
    "            sl = ' \\n'.join(sl) #sl is a list of segments so we need to join them before indexing (with \\n and a space)\n",
    "            sl = sl.split(' ') #then split them into words by white space (since \\n was with a space so its okay)\n",
    "            sl = sl[start_diff:] #select the required words\n",
    "            sl = ' '.join(sl) #join them with spaces\n",
    "            sstr = sl\n",
    "        elif start_diff<0:\n",
    "            print('seg more than topic error')\n",
    "            break\n",
    "        else:\n",
    "            sstr = '\\n'.join(sl)\n",
    "\n",
    "        #enter words for that sub_topic in dataframe\n",
    "        tdf.loc[i,'msg'] = sstr\n",
    "        \n",
    "    ##################################################################################################################\n",
    "    # combine the text for each sub_topic to create a complete text file\n",
    "    ##################################################################################################################\n",
    "    \n",
    "    #joining topics\n",
    "    tdf = tdf[~(tdf.msg == '')]\n",
    "    #converting subtopics to root topics\n",
    "    tdf.loc[~tdf.topic.isin(root_topics), 'topic'] = tdf.loc[~tdf.topic.isin(root_topics), 'topic'].apply(lambda x: int(parent_dict[str(x)]))\n",
    "    \n",
    "    #creating text variable\n",
    "    text = ''\n",
    "    #grouping all subtopics and segments of each root topic\n",
    "    sub_topic_text = tdf.groupby(by='topic', sort=False)['msg'].agg(lambda x: '\\n'.join(x))\n",
    "    #concatinating all the root topic texts with 10 asterics\n",
    "    for msg in sub_topic_text:\n",
    "        text = text + msg + '\\n' + '**********' + '\\n'\n",
    "    #remove any multiple adjacent line changes due to segment being empty\n",
    "    text = re.subn(r'[^\\S\\n]{2,}', ' ', text)[0] #remove double or more spaces except for \\n\n",
    "    text = re.subn('\\n\\s+', '\\n', text)[0] #remove any white space after line change\n",
    "    text = re.subn('\\s+\\n', '\\n', text)[0] #remove any white space before line change\n",
    "    #test = re.subn(' {2,}', ' ', text)[0] #remove double or more spaces\n",
    "    text = re.subn('\\n{2,}', '\\n', text)[0] #remove double or more line changes\n",
    "    text = re.subn(r'[^\\S\\n]{2,}', ' ', text)[0] #remove double or more spaces AGAIN\n",
    "    text = re.subn(r'\\n(?=[^\\*])', '\\n ', text)[0] #add spaces for the start of the segments\n",
    "    text = ' ' + text #add space for the first segment\n",
    "    text = text.rstrip()\n",
    "    \n",
    "    \n",
    "    #return the text of the topic\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Files Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rizvi\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:537: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ES2002a --created\n",
      "ES2002b --created\n",
      "ES2002c --created\n",
      "ES2002d --created\n",
      "ES2003a --created\n",
      "ES2003b --created\n",
      "ES2003c --created\n",
      "ES2003d --created\n",
      "ES2004a --created\n",
      "ES2004b --created\n",
      "ES2004c --created\n",
      "ES2004d --created\n",
      "ES2005a --created\n",
      "ES2005b --created\n",
      "ES2005c --created\n",
      "ES2005d --created\n",
      "ES2006a --created\n",
      "ES2006b --created\n",
      "ES2006d --created\n",
      "ES2007a --created\n",
      "ES2007b --created\n",
      "ES2007c --created\n",
      "ES2007d --created\n",
      "ES2008a --created\n",
      "ES2008b --created\n",
      "ES2008c --created\n",
      "ES2008d --created\n",
      "ES2009a --created\n",
      "ES2009b --created\n",
      "ES2009c --created\n",
      "ES2009d --created\n",
      "ES2010a --created\n",
      "ES2010b --created\n",
      "ES2010c --created\n",
      "ES2010d --created\n",
      "ES2011a --created\n",
      "ES2011b --created\n",
      "ES2011c --created\n",
      "ES2011d --created\n",
      "ES2012a --created\n",
      "ES2012b --created\n",
      "ES2012c --created\n",
      "ES2012d --created\n",
      "ES2013a --created\n",
      "ES2013b --created\n",
      "ES2013c --created\n",
      "ES2013d --created\n",
      "ES2014a --created\n",
      "ES2014b --created\n",
      "ES2014c --created\n",
      "ES2014d --created\n",
      "ES2015a --created\n",
      "ES2015d --created\n",
      "ES2016a --created\n",
      "ES2016b --created\n",
      "ES2016c --created\n",
      "ES2016d --created\n",
      "IB4003 --created\n",
      "IB4005 --created\n",
      "IB4010 --created\n",
      "IB4011 --created\n",
      "IS1000a --created\n",
      "IS1000b --created\n",
      "IS1000c --created\n",
      "IS1000d --created\n",
      "IS1001a --created\n",
      "IS1001b --created\n",
      "IS1001c --created\n",
      "IS1001d --created\n",
      "IS1002b --created\n",
      "IS1002c --created\n",
      "IS1002d --created\n",
      "IS1003a --created\n",
      "IS1003b --created\n",
      "IS1003c --created\n",
      "IS1003d --created\n",
      "IS1004a --created\n",
      "IS1004b --created\n",
      "IS1004c --created\n",
      "IS1004d --created\n",
      "IS1005a --created\n",
      "IS1005b --created\n",
      "IS1005c --created\n",
      "IS1006a --created\n",
      "IS1006b --created\n",
      "IS1006c --created\n",
      "IS1006d --created\n",
      "IS1007a --created\n",
      "IS1007b --created\n",
      "IS1007c --created\n",
      "IS1007d --created\n",
      "IS1008a --created\n",
      "IS1008b --created\n",
      "IS1008c --created\n",
      "IS1008d --created\n",
      "IS1009a --created\n",
      "IS1009b --created\n",
      "IS1009c --created\n",
      "IS1009d --created\n",
      "TS3003a --created\n",
      "TS3003b --created\n",
      "TS3003c --created\n",
      "TS3003d --created\n",
      "TS3004a --created\n",
      "TS3004b --created\n",
      "TS3004c --created\n",
      "TS3004d --created\n",
      "TS3005a --created\n",
      "TS3005b --created\n",
      "TS3005c --created\n",
      "TS3005d --created\n",
      "TS3006a --created\n",
      "TS3006b --created\n",
      "TS3006c --created\n",
      "TS3006d --created\n",
      "TS3007a --created\n",
      "TS3007b --created\n",
      "TS3007c --created\n",
      "TS3007d --created\n",
      "TS3008a --created\n",
      "TS3008b --created\n",
      "TS3008c --created\n",
      "TS3008d --created\n",
      "TS3009a --created\n",
      "TS3009b --created\n",
      "TS3009c --created\n",
      "TS3009d --created\n",
      "TS3010a --created\n",
      "TS3010b --created\n",
      "TS3010c --created\n",
      "TS3010d --created\n",
      "TS3011a --created\n",
      "TS3011b --created\n",
      "TS3011c --created\n",
      "TS3011d --created\n",
      "TS3012a --created\n",
      "TS3012b --created\n",
      "TS3012c --created\n",
      "TS3012d --created\n"
     ]
    }
   ],
   "source": [
    "#list to store tuples\n",
    "\n",
    "#folder to check\n",
    "xml_file_path = './topics'\n",
    "\n",
    "#all topic files\n",
    "files = os.listdir(xml_file_path)\n",
    "\n",
    "#for each topic file \n",
    "for xfile in files:\n",
    "    \n",
    "#     if xfile != 'ES2002d.topic.xml':\n",
    "#         continue\n",
    "    \n",
    "    ###################################################################################################################\n",
    "    # For that topic now create sub_Topics\n",
    "    ###################################################################################################################\n",
    "    \n",
    "    #identify topic id (this will be used to get the required segments and words)\n",
    "    topic_id = re.search(r'[^.]*(?=\\.)',xfile).group(0)\n",
    "    \n",
    "    #folder to check\n",
    "    xml_file_path = './topics'\n",
    "    \n",
    "    #create the path to file\n",
    "    xfile = os.path.join(xml_file_path, xfile)\n",
    "    \n",
    "    #if the file is a correct file and is .xml\n",
    "    if os.path.isfile(xfile) and xfile.endswith('.xml'): \n",
    "        #create the tuples\n",
    "        tops, root_topics, parent_dict = parse_topic(open(xfile))\n",
    "        \n",
    "        \n",
    "    ###################################################################################################################\n",
    "    # For that topic now create sugments\n",
    "    ###################################################################################################################\n",
    "        \n",
    "    #list to store tuples\n",
    "    segs = []\n",
    "    #folder to check\n",
    "    xml_file_path = './segments'\n",
    "\n",
    "    #all segment files\n",
    "    files = os.listdir(xml_file_path)\n",
    "\n",
    "    #filter segments files by topic\n",
    "    regex = re.compile(topic_id)\n",
    "    selected_files = filter(regex.search, files)\n",
    "\n",
    "\n",
    "    #for each segment file of that topic\n",
    "    for xfile in selected_files:\n",
    "\n",
    "        #create the path to file\n",
    "        xfile = os.path.join(xml_file_path, xfile)\n",
    "\n",
    "\n",
    "        #if the file is a correct file and is .xml\n",
    "        if os.path.isfile(xfile) and xfile.endswith('.xml'): \n",
    "\n",
    "            #create the tuples\n",
    "            segment_part = parse_seg(open(xfile))\n",
    "            segs = segs + segment_part\n",
    "            \n",
    "    ###################################################################################################################\n",
    "    # For that topic now create words\n",
    "    ###################################################################################################################\n",
    "    \n",
    "    #list to store tuples\n",
    "    wors = []\n",
    "    #folder to check\n",
    "    xml_file_path = './words'\n",
    "\n",
    "    #all segment files\n",
    "    files = os.listdir(xml_file_path)\n",
    "\n",
    "\n",
    "    #filter word files by topic\n",
    "    regex = re.compile(topic_id)\n",
    "    selected_files = filter(regex.search, files)\n",
    "\n",
    "    #for each word file of that topic\n",
    "    for xfile in selected_files:\n",
    "\n",
    "        #create the path to file\n",
    "        xfile = os.path.join(xml_file_path, xfile)\n",
    "\n",
    "        #if the file is a correct file and is .xml\n",
    "        if os.path.isfile(xfile) and xfile.endswith('.xml'): \n",
    "\n",
    "            #create the tuples\n",
    "            word_part = parse_words(open(xfile))\n",
    "            wors = wors + word_part\n",
    "       \n",
    "    ###################################################################################################################\n",
    "    # once the tuples for all information from the xml is now taken, create a dataframe from tuples\n",
    "    ###################################################################################################################     \n",
    "    \n",
    "    #create dataframes\n",
    "    #create dataframe for words\n",
    "    wdf = pd.DataFrame(wors, columns=['segment', 'ind', 'value']) \n",
    "    \n",
    "    #create dataframe for topics\n",
    "    tdf = pd.DataFrame(tops, columns=['topic', 'segment', 'start', 'stop'])\n",
    "    #since we used regex some subtopics repeat hence delete the duplicates\n",
    "    tdf.drop_duplicates(keep='first', inplace=True)\n",
    "    #reset the index after duplicate deletion\n",
    "    tdf.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    #create data frame for segments\n",
    "    sdf = pd.DataFrame(segs, columns=['segment', 'start', 'stop'])\n",
    "    \n",
    "    #code checking purposes\n",
    "#     print(topic_id)\n",
    "#     print(len(tdf),':tdf')\n",
    "#     print(len(sdf),':sdf')\n",
    "#     print(len(wdf),':wdf')\n",
    "#     print(tops[0:5])\n",
    "#     print(tdf.head())\n",
    "#     print(sdf[sdf.segment=='D'].head())\n",
    "#     print('---end---')\n",
    "        \n",
    "    ###################################################################################################################\n",
    "    # Create the text representation of topic and save in file\n",
    "    ################################################################################################################### \n",
    "    \n",
    "    Text = topic_to_text(root_topics, parent_dict, tdf, sdf, wdf)   \n",
    "    \n",
    "    # Open a file to store output in for the topic\n",
    "    fi = open('./txt_files/'+ topic_id + \".txt\", \"w\")\n",
    "    fi.write(Text)\n",
    "\n",
    "    # Close opend file\n",
    "    fi.close()\n",
    "    print(topic_id + ' --created')\n",
    "    \n",
    "    #empty the topic id\n",
    "    topic_id = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since there are alot of input and output operations needed as well as alot of information that needed to be stored, the code made use of pandas data frames. \n",
    "- However, the optimality of the code could have been increase if for example dictionaries were used. In this case, due to the shortage of time, pandas was used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
