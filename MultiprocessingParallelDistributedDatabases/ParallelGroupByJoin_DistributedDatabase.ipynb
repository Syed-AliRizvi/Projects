{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d67L_46djeOd"
   },
   "source": [
    "# Distributed Databases and Big Data\n",
    "\n",
    "# Solution Workbook\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P0M0i4UzrYF9"
   },
   "source": [
    "**About Jupyter Notebook**\n",
    "\n",
    "*Server Information:*\n",
    "\n",
    "    The version of the notebook server is 4.2.3 and is running on:\n",
    "        Python 3.5.2 \n",
    "\n",
    "\n",
    "*Current Kernel Information:*\n",
    "    \n",
    "    Python 3.5.2 \n",
    "    IPython 5.1.0 \n",
    "\n",
    "*Kindly run .ipynb file from the folder extracted from zip inplace, or by opening the file in jupyter by directing it to the mentioned folder.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "  \n",
    "- [Parallel Group By Join](#5)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JAUECcgOjeOf"
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IFlb2faEmlU0"
   },
   "source": [
    "***`If the library isn't installed already, please unhash and run the code below.`***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "FPlNIFz1kZq0"
   },
   "outputs": [],
   "source": [
    "#!pip install pandas\n",
    "#!pip install multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "_UbjOFqgjeOg"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0hRlUjcVjeOk"
   },
   "source": [
    "## data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Ur_LwBKfjeOl"
   },
   "outputs": [],
   "source": [
    "# CLIMATE DATA\n",
    "\n",
    "#read excel into dataframe\n",
    "cdf = pd.read_csv('ClimateData.csv')\n",
    "\n",
    "#create list\n",
    "cd = []\n",
    "row = []\n",
    "for i in range(cdf.shape[0]):\n",
    "    for j in range(cdf.shape[1]):\n",
    "        row.append(cdf.iloc[i,j])\n",
    "    cd.append(row)\n",
    "    row = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "UH9Esnv9MeFw"
   },
   "outputs": [],
   "source": [
    "# FIRE DATA \n",
    "\n",
    "#read excel into dataframe\n",
    "fdf = pd.read_csv('FireData.csv')\n",
    "\n",
    "#creating list\n",
    "fd = []\n",
    "row = []\n",
    "for i in range(fdf.shape[0]):\n",
    "    for j in range(fdf.shape[1]):\n",
    "        row.append(fdf.iloc[i,j])\n",
    "    fd.append(row)\n",
    "    row = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Group By Join\n",
    "\n",
    "<a id='5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4AT3X9b8jeRx"
   },
   "source": [
    "# ** Parallel Group-By Join **\n",
    "1. Write an algorithm to find the ​ average surface temperature ​ ​ (°C) ​ for each weather station.\n",
    "You are required to only display ​ average surface temperature (°C) ​ ​ and ​ the station ​ in the\n",
    "output.​ ​ Justify your choice of the data partition and join technique.\n",
    "Hint: You need to join using the date and group by based on station.\n",
    "\n",
    "**Solution:** Group by Partitioning Algorithem, Range Partition will be done in it.\n",
    "\n",
    "Since the Join attribute is different than the group by attribute hence one of the Parallel Group by **`after`** Join algorithems will need to be used. In this case the Group by Partitioning Algorithem is used rather than the Join Partitioning as the former is a one phase partitioning scheme while the latter is a two phased one. \n",
    "\n",
    "The group by attribute here are the stations. Keeping in mind that there are only three unique stations the range partitioning will be done on 3 processors with each processor getting one station. This might cause a bit of skew but would still be better than the join partition algorithem because of two reasons:\n",
    "\n",
    "* The aggregate function is 'average' and hence during the redistribution phase on the group by attribute in join partition algorithem extra information such as the count of values for each aggreate station will also need to be provided. This adds complexity to the algorithem. \n",
    "\n",
    "* Second join partition is a two phase method which means it adds an extra layer of processing for the processors and also includes transfering data between the processors a total of two times; once based on the join attribute and the second based on the group by attribute.\n",
    "\n",
    "In terms of joining the tables hash join will be used as it is the most efficient compared to nested loop and sort merge. Since the grouping/aggregating is done locally in each processor hence there is no parallel algorithm that needs to be used. \n",
    "\n",
    "Also, when applying the group by partition algorithem we will use the climate data set to be hashed and fire data to be probed. This is because the climate data is much smaller than the fire data hence this will result in a smaller hash table. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QeKzTcAOjeRy"
   },
   "source": [
    "### Range partitioning Function to be used to range partition the Table with group by attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "G6ZrPiIZjeRy"
   },
   "outputs": [],
   "source": [
    "# Range data partitionining function\n",
    "def range_partition(data, range_indices):\n",
    "    \n",
    "    \"\"\"\n",
    "    Perform range data partitioning on data\n",
    "    *Based on partitioning the climate data on station\n",
    "    Arguments:\n",
    "    data -- an input dataset which is a list\n",
    "    range_indices -- the index list of ranges to be split \n",
    "    Return:\n",
    "    result -- the paritioned subsets of D\n",
    "    \"\"\"\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    \n",
    "    # First, we sort the dataset according their values\n",
    "    new_data = sorted(data, key= lambda x: x[0])\n",
    "    \n",
    "    \n",
    "    # Calculate the number of bins - 1\n",
    "    n_bin = len(range_indices)\n",
    "    \n",
    "    \n",
    "    # For each bin, perform the following\n",
    "    for i in range(n_bin):\n",
    "        \n",
    "        # Find elements to be belonging to each range\n",
    "        s = [x for x in new_data if x[0] < range_indices[i]]\n",
    "        \n",
    "        # Add the partitioned list to the result\n",
    "        result.append(s)\n",
    "        \n",
    "        # Find the last element in the previous partition\n",
    "        last_element = s[len(s)-1]\n",
    "        \n",
    "        # Find the index of of the last element\n",
    "        last = new_data.index(last_element)\n",
    "        \n",
    "        # Remove the partitioned list from the dataset\n",
    "        new_data = new_data[int(last)+1:]\n",
    "    \n",
    "    # Append the last remaining data list\n",
    "    result.append([x for x in new_data if x[0] >= range_indices[n_bin-1]])\n",
    "    \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "CsngF199jeRz",
    "outputId": "ae59ed8c-1c5a-4410-abae-f7bedf579a2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[948700, '2016-12-31', 19, 56.8, 7.9, 11.1, '   72.0*', '  61.9*', ' 0.00I']]\n",
      "[[948701, '2016-12-31', 19, 56.8, 7.9, 11.1, '   72.0*', '  61.9*', ' 0.00I']]\n",
      "[[948702, '2016-12-31', 19, 56.8, 7.9, 11.1, '   72.0*', '  61.9*', ' 0.00I']]\n"
     ]
    }
   ],
   "source": [
    "#testing range partition\n",
    "\n",
    "a = [948702, '2016-12-31', 19, 56.799999999999997, 7.9000000000000004, 11.1, '   72.0*', '  61.9*', ' 0.00I']\n",
    "b = [948700, '2016-12-31', 19, 56.799999999999997, 7.9000000000000004, 11.1, '   72.0*', '  61.9*', ' 0.00I']\n",
    "c = [948701, '2016-12-31', 19, 56.799999999999997, 7.9000000000000004, 11.1, '   72.0*', '  61.9*', ' 0.00I']\n",
    "\n",
    "l = [a, b, c]\n",
    "\n",
    "out = range_partition(l, [948701, 948702])\n",
    "print(out[0])\n",
    "print(out[1])\n",
    "print(out[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZgHuApNQjeR1"
   },
   "source": [
    "### Hash Join function for local join in each processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I7VCd9FojeR2"
   },
   "source": [
    "##### hash key function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "s2fkblbEjeR2"
   },
   "outputs": [],
   "source": [
    "# Define a simple hash function.\n",
    "def s_hash(x, n):\n",
    "    \"\"\"\n",
    "    Define a simple hash function for demonstration\n",
    "    Arguments:\n",
    "    x -- an input date as string\n",
    "    n -- the number of processors\n",
    "    Return:\n",
    "    result -- the hash value of x\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    date_parse = x.split('-')\n",
    "    result = int(date_parse[2])%n #hash key by days\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "IVoHjfB6jeR3",
    "outputId": "d0a65b78-c48d-4721-de0a-09b52d59718c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test hash\n",
    "s_hash('2016-12-31',3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5_LwaZCOjeR6"
   },
   "source": [
    "##### Hash based join function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "tDKBRaH3jeR7"
   },
   "outputs": [],
   "source": [
    "def HB_join(T1, T2, n_processor):\n",
    "    \n",
    "    \"\"\"\n",
    "    Perform the hash-based join algorithm.\n",
    "    The join attribute is the numeric attribute in the input tables T1 & T\n",
    "    2\n",
    "    Arguments:\n",
    "    T1 & T2 -- Tables to be joined\n",
    "    n_processor -- number of processors to be used\n",
    "    Return:\n",
    "    result -- the joined table\n",
    "    \"\"\"\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    dic = {} # We will use a dictionary\n",
    "    \n",
    "    # For each record in table T2\n",
    "    for rec in T2:\n",
    "        \n",
    "        # Hash the record based on join attribute value using hash function H into hash table\n",
    "        s_key = s_hash(rec[1], n_processor)\n",
    "        \n",
    "        dic.setdefault(s_key, []) #add key is doesnt exist if it does let it be\n",
    "        dic[s_key].append(rec[0:2]) #add record\n",
    "    \n",
    "    # For each record in table T1 (probing)\n",
    "    for rec in T1:\n",
    "        # Hash the record based on join attribute value using H\n",
    "        r_key = s_hash(rec[6], n_processor)\n",
    "        \n",
    "        if r_key in dic: # If an index entry is found Then            \n",
    "            for value in dic[r_key]:   # Compare each record on this index entry with the record of table T1         \n",
    "                if value[1] == rec[6]:\n",
    "                    result.append([value[0]] + [rec[-1]])      \n",
    "    \n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "lgIstS5rjeR8"
   },
   "outputs": [],
   "source": [
    "#test hb_join\n",
    "#HB_join(fd, cd, 3) #uncomment to see result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HWWEhe32jeR9"
   },
   "source": [
    "### Parallel Group By Partitioning Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z6pUMJOBjeR9"
   },
   "source": [
    "##### local group by function to be used to calculate average for each station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "m65wcjwCjeR9"
   },
   "outputs": [],
   "source": [
    "#local group by function for avg\n",
    "def local_group_by_avg(data):\n",
    "    \n",
    "    #create list to store grouped data\n",
    "    grouped_data = []\n",
    "    \n",
    "    #if data is empty return empty list\n",
    "    if len(data) == 0:\n",
    "        return grouped_data  \n",
    "    \n",
    "    #create dictionary to store values for each group by attribute\n",
    "    dic = {}\n",
    "    \n",
    "    #for each record put in its value under the group by attribute in the dictionary\n",
    "    for rec in data:\n",
    "        dic.setdefault(rec[0], [])\n",
    "        dic[rec[0]].append(rec[1])\n",
    "        \n",
    "    #for each key in dictionary calculate the sum and count of the values and then obtain the average\n",
    "    for key in dic.keys():\n",
    "        station_sum = sum(dic[key])\n",
    "        station_count = len(dic[key])\n",
    "        station_avg = station_sum/station_count\n",
    "        grouped_data.append([key, station_avg]) #append the grouped by data in the output list\n",
    "            \n",
    "    return grouped_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r5g-TD_gjeR-"
   },
   "source": [
    "##### Parallel group by partition function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Ks8MfK4ujeR_"
   },
   "outputs": [],
   "source": [
    "# Parallel searching algorithm for range selection\n",
    "# range partition and binary search\n",
    "\n",
    "def parallel_Group_by_Partition_Join_avg(T1, T2, n_processor, partition_range):\n",
    "    \n",
    "    \"\"\"\n",
    "    Perform parallel group by partition join and aggregates by avg\n",
    "\n",
    "    Arguments:\n",
    "    T1 & T2 -- the input datasets which need to be joined\n",
    "    T1: fire data\n",
    "    T2: climate data\n",
    "    n_processor -- the number of parallel processors\n",
    "    partition_range -- partition to be used for range partitioning phase (e.g. [30, 50])\n",
    "    \n",
    "    Return:\n",
    "    results -- joined and aggregated result \n",
    "    \"\"\"\n",
    "    \n",
    "    processes = [] #list to store all active processes\n",
    "    output = [] #list to store the output of all processes\n",
    "\n",
    "    pool = Pool(processes = n_processor+1) #one kept aside for the main python process and 3 for workers\n",
    "       \n",
    "    #Range partition climate data on station to 3 processors\n",
    "    T2_r_part_data = range_partition(T2, [948701, 948702])\n",
    "    \n",
    "    #in each processor perform local join of the ranged subset and the entire broadcasted T1\n",
    "    for T2_r_part in T2_r_part_data:\n",
    "        process = pool.apply_async(HB_join, args=(T1, T2_r_part, n_processor,))\n",
    "        processes.append(process)\n",
    "    \n",
    "    #obtain the joined data partitions in each process\n",
    "    joined_data_parts = [p.get() for p in processes]\n",
    "    processes = [] #empty active processes list\n",
    "    \n",
    "    #perform aggregate group by in each processor\n",
    "    for joined_data_part in joined_data_parts:\n",
    "        process = pool.apply_async(local_group_by_avg, args=(joined_data_part,))\n",
    "        processes.append(process)\n",
    "\n",
    "    #get output from each processor\n",
    "    joined_grouped_parts = [p.get() for p in processes]\n",
    "    processes = [] #empty active processors list\n",
    "    \n",
    "    #close the pool of processors\n",
    "    pool.close()\n",
    "    \n",
    "    #to obtain the final output just merge the data in all processors\n",
    "    for processor_data in joined_grouped_parts:\n",
    "        output = output + processor_data    \n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j-0rpPgijeSB"
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "o4m1CbuVjeSD",
    "outputId": "3de3989d-31dd-4df7-ef9c-0b969aa6cafd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[948701, 56.069386038687973], [948702, 52.148275862068964]]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using 3 processor and ranged so that each processor gets climate data for each station.\n",
    "parallel_Group_by_Partition_Join_avg(fd, cd, 3, [948701,948702])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5P_tE1CnjeSG"
   },
   "source": [
    "\n",
    "--------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "Assignment1.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
