{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Syed Ali Alim Rizvi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression vs. Bayesian Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in library(mvtnorm): there is no package called 'mvtnorm'\n",
     "output_type": "error",
     "traceback": [
      "Error in library(mvtnorm): there is no package called 'mvtnorm'\nTraceback:\n",
      "1. library(mvtnorm)",
      "2. stop(txt, domain = NA)"
     ]
    }
   ],
   "source": [
    "library(reshape2)\n",
    "library(ggplot2)\n",
    "library(tidyr)\n",
    "library(mvtnorm) # generates multivariate Gaussian sampels and calculate the densities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1e.train <- read.csv('LogisticVsBaysian_train.csv')\n",
    "t1e.test <- read.csv('LogisticVsBaysian_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps to Build a Logistic Regression\n",
    "Taking the following steps is neccesseary to build a logistic regression:\n",
    "<ol>\n",
    "\t<li>Implement sigmoid function $\\sigma(\\pmb{w}.\\mathbf{x})$, and initialize weight vector $\\pmb{w}$, learning rate $\\eta$ and stopping criterion $\\epsilon$.</li>\n",
    "\t<li>Repeat the followings until the improvement becomes negligible (i.e., $|\\mathcal{L}(\\pmb{w}^{(\\tau+1)})-\\mathcal{L}(\\pmb{w}^{(\\tau)})| \\lt \\epsilon$):\n",
    "<ol>\n",
    "\t<li>Shuffle the training data</li>\n",
    "\t<li>For each datapoint in the training data do:\n",
    "<ol>\n",
    "\t<li>$\\pmb{w}^{(\\tau+1)} := \\pmb{w}^{(\\tau)} - \\eta (\\sigma(\\pmb{w}.\\mathbf{x}) - t_n) \\pmb{x}_n$</li>\n",
    "</ol>\n",
    "</li>\n",
    "</ol>\n",
    "</li>\n",
    "</ol>\n",
    "\n",
    "In the followings, we implement each of these steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Linear Regression\n",
    "Similar to the previous activities, we first define some auxilary functions and then develop the logsitic regresion.\n",
    "### Auxilary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxiliary function that predicts class labels\n",
    "predict <- function(w, X, c0, c1){\n",
    "    sig <- sigmoid(w, X)\n",
    "    return(ifelse(sig>0.5, c1,c0))\n",
    "}\n",
    "    \n",
    "# auxiliary function that calculate a cost function\n",
    "cost <- function (w, X, T, c0){\n",
    "    sig <- sigmoid(w, X)\n",
    "    return(sum(ifelse(T==c0, 1-sig, sig)))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1 (Sigmoid):** Let's define our sigmoid function, first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sigmoid function (=p(C1|X))\n",
    "sigmoid <- function(w, x){\n",
    "    return(1.0/(1.0+exp(-w%*%t(cbind(1,x)))))    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1 (initializations):** Now, we initiate the weight vector, learning rate, stopping threshold, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializations\n",
    "tau.max <- 1000 # maximum number of iterations\n",
    "eta <- 0.01 # learning rate\n",
    "epsilon <- 0.01 # a threshold on the cost (to terminate the process)\n",
    "tau <- 1 # iteration counter\n",
    "terminate <- FALSE\n",
    "\n",
    "## Just a few name/type conversion to make the rest of the code easy to follow\n",
    "X <- as.matrix(train.data) # rename just for conviniance\n",
    "T <- ifelse(train.label==c0,0,1) # rename just for conviniance\n",
    "\n",
    "W <- matrix(,nrow=tau.max, ncol=(ncol(X)+1)) # to be used to store the estimated coefficients\n",
    "W[1,] <- runif(ncol(W)) # initial weight (any better idea?)\n",
    "\n",
    "# project data using the sigmoid function (just for convenient)\n",
    "Y <- sigmoid(W[1,],X)\n",
    "\n",
    "costs <- data.frame('tau'=1:tau.max)  # to be used to trace the cost in each iteration\n",
    "costs[1, 'cost'] <- cost(W[1,],X,T, c0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:** Here, we use SGD to learn the weight vector. Note that there are two loops. In the outter loop, we shuffle the samples and then start the inner loop. In the inner loop, we visit the training samples one by one and update the weights accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The  final coefficents are: 0.02115738 -0.9925501 1.041473"
     ]
    }
   ],
   "source": [
    "while(!terminate){\n",
    "    # check termination criteria:\n",
    "    terminate <- tau >= tau.max | cost(W[tau,],X,T, c0)<=epsilon\n",
    "    \n",
    "    # shuffle data:\n",
    "    train.index <- sample(1:train.len, train.len, replace = FALSE)\n",
    "    X <- X[train.index,]\n",
    "    T <- T[train.index]\n",
    "    \n",
    "    # for each datapoint:\n",
    "    for (i in 1:train.len){\n",
    "        # check termination criteria:\n",
    "        if (tau >= tau.max | cost(W[tau,],X,T, c0) <=epsilon) {terminate<-TRUE;break}\n",
    "        \n",
    "        Y <- sigmoid(W[tau,],X)\n",
    "            \n",
    "        # Update the weights\n",
    "        W[(tau+1),] <- W[tau,] - eta * (Y[i]-T[i]) * cbind(1, t(X[i,]))\n",
    "        \n",
    "        # record the cost:\n",
    "        costs[(tau+1), 'cost'] <- cost(W[tau,],X,T, c0)\n",
    "        \n",
    "        # update the counter:\n",
    "        tau <- tau + 1\n",
    "        \n",
    "        # decrease learning rate:\n",
    "        eta = eta * 0.999\n",
    "    }\n",
    "}\n",
    "# Done!\n",
    "costs <- costs[1:tau, ] # remove the NaN tail of the vector (in case of early stopping)\n",
    "\n",
    "# the  final result is:\n",
    "w <- W[tau,]\n",
    "cat('\\nThe  final coefficents are:',w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Classifier \n",
    "### Steps to Build A Bayesian Classifier\n",
    "These are the steps to build a bayesian Classifier:\n",
    "<ol>\n",
    "\t<li>Calculate the class priors $p(\\mathcal{C}_k)$ based on the relative number of training data in each class,</li>\n",
    "\t<li>Calculate the class means $\\mu_k$, class covariance matrices $\\mathbf{S}_k$ and shared covariance matrix $\\Sigma$ using the training data,</li>\n",
    "\t<li>Using the estimated PDF function, calculate $p(x_n|\\mathcal{C}_k)$ for each data point and each class,</li>\n",
    "\t<li>For each test sample, find the class label $\\mathcal{C}_k$ that maximizes the $p(\\mathcal{C}_k)p(x_n|\\mathcal{C}_k)$,</li>\n",
    "</ol>\n",
    "\n",
    "In the following we take these steps one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of Bayesian Classifier\n",
    "**Step 1:** Let's start with calculating the class probabilities and compare the obtained values with the real class probabilites.\n",
    "\n",
    "**Note:** we use `.hat` notation after the name of variables to differentiate the estimations from the original (generative parameter) values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The real class probabilities:\t\t0.600000, 0.400000\n",
      "The estimated class probabilities:\t0.612000, 0.388000\n"
     ]
    }
   ],
   "source": [
    "# Class probabilities:\n",
    "p0.hat <- sum(train.label==c0)/nrow(train.data) # total number of samples in class 0 divided by the total nmber of training data\n",
    "p1.hat <- sum(train.label==c1)/nrow(train.data) # or simply 1 - p1.hat\n",
    "\n",
    "cat(sprintf('\\nThe real class probabilities:\\t\\t%f, %f\\nThe estimated class probabilities:\\t%f, %f\\n', p0, p1, p0.hat, p1.hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2 (means):** Now, we estimate the class means and compare them with the real means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The real class means:\t\t4.500000, 0.500000\t1.000000, 4.000000\t\n",
      "The estimated class means:\t4.519526, 0.480368\t0.962417, 3.992787\t\n"
     ]
    }
   ],
   "source": [
    "# Class means:\n",
    "mu0.hat <- colMeans(train.data[train.label==c0,])\n",
    "mu1.hat <- colMeans(train.data[train.label==c1,])\n",
    "\n",
    "cat(sprintf('\\nThe real class means:\\t\\t%f, %f\\t%f, %f\\t\\nThe estimated class means:\\t%f, %f\\t%f, %f\\t\\n', \n",
    "          mu0[1], mu0[2], mu1[1], mu1[2],\n",
    "          mu0.hat[1], mu0.hat[2], mu1.hat[1], mu1.hat[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2 (variances):** Its time to calculate class variance matrices. Based on these matrices, we can easily calculate the shared covariance matrix as it is a weighted average of the class covariance matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The real class covariance matrix:\n",
      "\t1.000000, 0.000000\n",
      "\t0.000000, 1.000000\n",
      "The estimated covariance matrix:\n",
      "\t0.934979, 0.043186\n",
      "\t0.043186, 1.069280\n"
     ]
    }
   ],
   "source": [
    "# class covariance matrices:\n",
    "sigma0.hat <- var(train.data[train.label==c0,])\n",
    "sigma1.hat <- var(train.data[train.label==c1,])\n",
    "\n",
    "# shared covariance matrix:\n",
    "sigma.hat <- p0.hat * sigma0.hat + p1.hat * sigma1.hat \n",
    "\n",
    "cat(sprintf('\\nThe real class covariance matrix:\\n\\t%f, %f\\n\\t%f, %f\\nThe estimated covariance matrix:\\n\\t%f, %f\\n\\t%f, %f\\n', \n",
    "          sigma[1], sigma[2], sigma[3], sigma[4],\n",
    "          sigma.hat[1], sigma.hat[2], sigma.hat[3], sigma.hat[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:**  Now, we calculate the postoriors based on the estimated parameters in the above steps. \n",
    "\n",
    "**Note:** We can easily generate some samples using the learnt parameters. That's why Bayesian classifier is known as a generative model. As an optional actvity, try to generate some points, plot them and compare the visualizations with the original samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate posteriors:\n",
    "posterior0 <- p0.hat*dmvnorm(x=train.data, mean=mu0.hat, sigma=sigma.hat)\n",
    "posterior1 <- p1.hat*dmvnorm(x=train.data, mean=mu1.hat, sigma=sigma.hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4:** To predict the class label of a data pint, we only need to compare the posteriors and assign the label which is associated with te largest posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training accuracy:\t99.80%\n",
      "Testing accuracy:\t99.40%"
     ]
    }
   ],
   "source": [
    "# calculate predictions:\n",
    "train.predict <- ifelse(posterior0 > posterior1, c0, c1)\n",
    "test.predict <- ifelse(p0.hat*dmvnorm(x=test.data, mean=mu0.hat, sigma=sigma.hat) > p1.hat*dmvnorm(x=test.data, mean=mu1.hat, sigma=sigma.hat), c0, c1)\n",
    "\n",
    "# calculate accuracy:\n",
    "cat(sprintf('\\nTraining accuracy:\\t%.2f%%', sum(train.label==train.predict)/nrow(train.data)*100))\n",
    "cat(sprintf('\\nTesting accuracy:\\t%.2f%%', sum(test.label==test.predict)/nrow(test.data)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the performance of our classifier, let's produce two confusion matrices (one for training and the other one for testing data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in table(train.label, train.predict): object 'train.label' not found\n",
     "output_type": "error",
     "traceback": [
      "Error in table(train.label, train.predict): object 'train.label' not found\nTraceback:\n",
      "1. table(train.label, train.predict)"
     ]
    }
   ],
   "source": [
    "# Confusion Matix (Train):\n",
    "table(train.label, train.predict)\n",
    "\n",
    "# Confusion Matix (Test):\n",
    "table(test.label, test.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
